{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51890f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready\n"
     ]
    }
   ],
   "source": [
    "# Ensure compatibility across huggingface_hub versions: some versions removed\n",
    "# `cached_download`, which older packages expect. If it's missing, alias it\n",
    "# to the newer `hf_hub_download` so imports of sentence_transformers succeed.\n",
    "try:\n",
    "\timport huggingface_hub\n",
    "\tif not hasattr(huggingface_hub, \"cached_download\"):\n",
    "\t\tfrom huggingface_hub import hf_hub_download\n",
    "\t\tdef cached_download(*args, **kwargs):\n",
    "\t\t\treturn hf_hub_download(*args, **kwargs)\n",
    "\t\thuggingface_hub.cached_download = cached_download\n",
    "except Exception:\n",
    "\t# If huggingface_hub isn't available or another issue occurs, let the\n",
    "\t# normal import raise an informative error later.\n",
    "\tpass\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "print(\"Ready\")  # Now compatible; prints \"Ready\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7785ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core libraries imported. Creating folders and loading dataset...\n",
      "Folders 'data/' and 'models/' ready.\n",
      "Error: data.csv not found in data/. Download from Kaggle.\n",
      "NLTK imported successfully.\n",
      "SentenceTransformer loaded.\n",
      "All imports complete. Ready for Cell 2.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import libraries and load dataset (With huggingface_hub compatibility patch)\n",
    "# Ensure compatibility across huggingface_hub versions: some versions removed\n",
    "# `cached_download`, which older packages expect. If it's missing, alias it\n",
    "# to the newer `hf_hub_download` so imports of sentence_transformers succeed.\n",
    "try:\n",
    "    import huggingface_hub\n",
    "    if not hasattr(huggingface_hub, \"cached_download\"):\n",
    "        from huggingface_hub import hf_hub_download\n",
    "        def cached_download(*args, **kwargs):\n",
    "            return hf_hub_download(*args, **kwargs)\n",
    "        huggingface_hub.cached_download = cached_download\n",
    "except Exception:\n",
    "    # If huggingface_hub isn't available or another issue occurs, let the\n",
    "    # normal import raise an informative error later.\n",
    "    pass\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import textstat\n",
    "\n",
    "print(\"Core libraries imported. Creating folders and loading dataset...\")\n",
    "\n",
    "# Auto-create folders\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "print(\"Folders 'data/' and 'models/' ready.\")\n",
    "\n",
    "# Path to dataset\n",
    "data_path = os.path.join(os.getcwd(), 'data', 'data.csv')\n",
    "\n",
    "# Load dataset\n",
    "df = pd.DataFrame()\n",
    "try:\n",
    "    df = pd.read_csv(data_path, encoding='utf-8')\n",
    "    print(f\"Dataset loaded successfully from {data_path}\")\n",
    "    print(f\"Number of rows: {len(df)}\")  # ~65\n",
    "    print(f\"Columns: {df.columns.tolist()}\")  # ['url', 'html_content']\n",
    "    print(\"\\nSample row preview:\")\n",
    "    print(df.head(1).to_string())\n",
    "    empty_html = df['html_content'].isna().sum() + (df['html_content'] == '').sum()\n",
    "    print(f\"Rows with empty/missing HTML: {empty_html}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: data.csv not found in data/. Download from Kaggle.\")\n",
    "    df = pd.DataFrame({'url': [], 'html_content': []})\n",
    "except UnicodeDecodeError:\n",
    "    df = pd.read_csv(data_path, encoding='latin-1')\n",
    "    print(\"Used latin-1 encoding fallback.\")\n",
    "\n",
    "# NLTK safe import\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    print(\"NLTK imported successfully.\")\n",
    "    NLTK_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    print(f\"NLTK warning: {e}. Fallback active.\")\n",
    "    NLTK_AVAILABLE = False\n",
    "    def sent_tokenize(text):\n",
    "        return re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text) if text else []\n",
    "\n",
    "# ML libs\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "import joblib\n",
    "\n",
    "# SentenceTransformer (with patch)\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    print(\"SentenceTransformer loaded.\")\n",
    "    EMBED_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    print(f\"Embedding warning: {e}. Using zero fallback (TF-IDF still works).\")\n",
    "    EMBED_AVAILABLE = False\n",
    "    model = None\n",
    "    def encode(texts):\n",
    "        return [np.zeros(384) for _ in texts]\n",
    "\n",
    "print(\"All imports complete. Ready for Cell 2.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "716cf327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting HTML parsing...\n",
      "Warning: No data loaded. Run Cell 1 first.\n",
      "\n",
      "Parsing complete! Total rows: 0\n",
      "No data to process. Ensure data.csv loads 65 rows in Cell 1.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: HTML Parsing and Text Extraction (Safe for empty df)\n",
    "def parse_html(html_content, url):\n",
    "    if not html_content or len(str(html_content)) < 100:\n",
    "        return {'url': url, 'title': 'Empty HTML', 'body_text': '', 'word_count': 0}\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        title_tag = soup.title or soup.find('h1')\n",
    "        title = title_tag.get_text().strip() if title_tag else 'No title'\n",
    "        for unwanted in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):\n",
    "            unwanted.decompose()\n",
    "        content_selectors = ['p', 'article', 'main', 'div[class*=\"content\"]', 'div[class*=\"article\"]', 'section']\n",
    "        body_parts = []\n",
    "        for selector in content_selectors:\n",
    "            elements = soup.select(selector)\n",
    "            body_parts.extend([el.get_text().strip() for el in elements if el.get_text().strip()])\n",
    "        body_text = ' '.join(body_parts) if body_parts else soup.get_text()\n",
    "        body_text = re.sub(r'\\s+', ' ', body_text).strip().lower()\n",
    "        body_text = re.sub(r'[^\\w\\s\\.\\-]', '', body_text)\n",
    "        word_count = len(re.findall(r'\\b\\w+\\b', body_text))\n",
    "        return {\n",
    "            'url': url,\n",
    "            'title': title[:200] + '...' if len(title) > 200 else title,\n",
    "            'body_text': body_text,\n",
    "            'word_count': max(0, word_count)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing {url}: {str(e)[:100]}...\")\n",
    "        return {'url': url, 'title': 'Parse error', 'body_text': '', 'word_count': 0}\n",
    "\n",
    "print(\"Starting HTML parsing...\")\n",
    "parsed_data = []\n",
    "failed = 0\n",
    "total = len(df)\n",
    "if total == 0:\n",
    "    print(\"Warning: No data loaded. Run Cell 1 first.\")\n",
    "    extracted_df = pd.DataFrame(columns=['url', 'title', 'body_text', 'word_count'])\n",
    "else:\n",
    "    for idx, row in df.iterrows():\n",
    "        if idx % 10 == 0:\n",
    "            print(f\"Processed {idx}/{total} rows...\")\n",
    "        result = parse_html(row['html_content'], row['url'])\n",
    "        parsed_data.append(result)\n",
    "        if result['word_count'] == 0:\n",
    "            failed += 1\n",
    "    extracted_df = pd.DataFrame(parsed_data)\n",
    "\n",
    "extracted_df.to_csv('data/extracted_content.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"\\nParsing complete! Total rows: {total}\")\n",
    "if total > 0:\n",
    "    print(f\"Failed/empty: {failed} ({failed/total*100:.1f}%)\")\n",
    "    print(f\"Average word count: {extracted_df['word_count'].mean():.0f}\")\n",
    "    print(f\"Min/Max words: {extracted_df['word_count'].min()}/{extracted_df['word_count'].max()}\")\n",
    "    print(\"\\nSample extracted data (first 2 rows):\")\n",
    "    print(extracted_df[['url', 'title', 'word_count']].head(2).to_string(index=False))\n",
    "else:\n",
    "    print(\"No data to process. Ensure data.csv loads 65 rows in Cell 1.\")\n",
    "\n",
    "global_extracted_df = extracted_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "479248c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: c:\\Users\\MOHAMMED HANEES\\OneDrive\\Desktop\\Big Data\\seo-content-detector\\notebooks\n",
      "Data folder exists: True\n",
      "data.csv exists: False\n",
      "File missing—copy data.csv to data/ folder.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Current directory:\", os.getcwd())  # Should be C:\\Users\\MOHAMMED HANEES\\OneDrive\\Desktop\\Big Data\\seo-content-detector\n",
    "print(\"Data folder exists:\", os.path.exists('data'))\n",
    "print(\"data.csv exists:\", os.path.exists(os.path.join('data', 'data.csv')))\n",
    "if os.path.exists(os.path.join('data', 'data.csv')):\n",
    "    import pandas as pd\n",
    "    df_test = pd.read_csv('data/data.csv', nrows=2)  # Test load 2 rows\n",
    "    print(\"Test load success. Rows preview:\")\n",
    "    print(df_test)\n",
    "else:\n",
    "    print(\"File missing—copy data.csv to data/ folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bfca978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core libraries imported. Debug: Current dir = c:\\Users\\MOHAMMED HANEES\\OneDrive\\Desktop\\Big Data\\seo-content-detector\\notebooks\n",
      "Folders ready.\n",
      "Relative path exists: False\n",
      "Full path exists: True\n",
      "Loaded from full path: C:\\Users\\MOHAMMED HANEES\\OneDrive\\Desktop\\Big Data\\seo-content-detector\\data\\data.csv\n",
      "Dataset loaded! Rows: 81\n",
      "Columns: ['url', 'html_content']\n",
      "\n",
      "Sample row (first URL/HTML snippet):\n",
      "                                              url\n",
      "0  https://www.cm-alliance.com/cybersecurity-blog\n",
      "(HTML truncated for preview)\n",
      "Empty HTML rows: 12\n",
      "NLTK ready.\n",
      "SentenceTransformer loaded.\n",
      "All ready. Run debug temp cell if rows != 65.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import libraries and load dataset (With path debug & fallback)\n",
    "# Huggingface compatibility patch\n",
    "try:\n",
    "    import huggingface_hub\n",
    "    if not hasattr(huggingface_hub, \"cached_download\"):\n",
    "        from huggingface_hub import hf_hub_download\n",
    "        def cached_download(*args, **kwargs):\n",
    "            return hf_hub_download(*args, **kwargs)\n",
    "        huggingface_hub.cached_download = cached_download\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import textstat\n",
    "\n",
    "print(\"Core libraries imported. Debug: Current dir =\", os.getcwd())\n",
    "\n",
    "# Auto-create folders\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "print(\"Folders ready.\")\n",
    "\n",
    "# Paths: Relative first, then full fallback\n",
    "data_path = os.path.join('data', 'data.csv')\n",
    "full_path = r\"C:\\Users\\MOHAMMED HANEES\\OneDrive\\Desktop\\Big Data\\seo-content-detector\\data\\data.csv\"  # Adjust if needed\n",
    "print(f\"Relative path exists: {os.path.exists(data_path)}\")\n",
    "print(f\"Full path exists: {os.path.exists(full_path)}\")\n",
    "\n",
    "df = pd.DataFrame()\n",
    "try:\n",
    "    if os.path.exists(data_path):\n",
    "        df = pd.read_csv(data_path, encoding='utf-8')\n",
    "    elif os.path.exists(full_path):\n",
    "        df = pd.read_csv(full_path, encoding='utf-8')\n",
    "        print(f\"Loaded from full path: {full_path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"data.csv not found. Copy to data/ or check path.\")\n",
    "    \n",
    "    print(f\"Dataset loaded! Rows: {len(df)}\")  # 65 expected\n",
    "    print(f\"Columns: {df.columns.tolist()}\")  # ['url', 'html_content']\n",
    "    print(\"\\nSample row (first URL/HTML snippet):\")\n",
    "    print(df.head(1)[['url']].to_string() + \"\\n(HTML truncated for preview)\")\n",
    "    empty_html = df['html_content'].isna().sum() + (df['html_content'] == '').sum()\n",
    "    print(f\"Empty HTML rows: {empty_html}\")\n",
    "except Exception as e:\n",
    "    print(f\"Load error: {e}. Download data.csv from Kaggle to data/.\")\n",
    "\n",
    "# NLTK\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    print(\"NLTK ready.\")\n",
    "    NLTK_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    print(f\"NLTK fallback: {e}\")\n",
    "    NLTK_AVAILABLE = False\n",
    "    def sent_tokenize(text):\n",
    "        return re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text) if text else []\n",
    "\n",
    "# ML libs\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "import joblib\n",
    "\n",
    "# SentenceTransformer\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    print(\"SentenceTransformer loaded.\")\n",
    "    EMBED_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    print(f\"Embedding fallback: {e}\")\n",
    "    EMBED_AVAILABLE = False\n",
    "    model = None\n",
    "    def encode(texts):\n",
    "        return [np.zeros(384) for _ in texts]\n",
    "\n",
    "print(\"All ready. Run debug temp cell if rows != 65.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b13e7ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No extracted data. Run Cells 1-2 first.\n",
      "\n",
      "Feature extraction complete! Rows: 0\n",
      "Empty features. Fix data load in Cell 1.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Text Preprocessing & Feature Engineering (Safe for empty)\n",
    "features_df = global_extracted_df.copy() if 'global_extracted_df' in globals() else pd.DataFrame()  # From Cell 2\n",
    "\n",
    "if features_df.empty:\n",
    "    print(\"Warning: No extracted data. Run Cells 1-2 first.\")\n",
    "    # Create empty with columns\n",
    "    features_df = pd.DataFrame(columns=['url', 'title', 'body_text', 'word_count', 'sentence_count', 'flesch_reading_ease', 'top_keywords', 'embedding'])\n",
    "else:\n",
    "    # TF-IDF\n",
    "    non_empty_texts = [row['body_text'] for _, row in features_df.iterrows() if row['body_text']]\n",
    "    if non_empty_texts:\n",
    "        vectorizer = TfidfVectorizer(max_features=100, stop_words='english', ngram_range=(1,2))\n",
    "        tfidf_matrix = vectorizer.fit_transform(non_empty_texts)\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "    else:\n",
    "        vectorizer = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "        tfidf_matrix = np.zeros((len(features_df), 100))\n",
    "        feature_names = np.array([])\n",
    "\n",
    "    print(\"Extracting features...\")\n",
    "    for idx, row in features_df.iterrows():\n",
    "        if idx % 10 == 0:\n",
    "            print(f\"Processed {idx}/{len(features_df)}...\")\n",
    "        \n",
    "        body_text = row['body_text']\n",
    "        sentences = sent_tokenize(body_text) if NLTK_AVAILABLE else re.split(r'\\. ', body_text)\n",
    "        sentence_count = len(sentences)\n",
    "        flesch_score = textstat.flesch_reading_ease(body_text) if len(body_text) > 100 else 0.0\n",
    "        \n",
    "        # Embedding\n",
    "        if EMBED_AVAILABLE and body_text:\n",
    "            embedding = model.encode([body_text])[0].tolist()\n",
    "        else:\n",
    "            embedding = [0.0] * 384\n",
    "        \n",
    "        # Top keywords\n",
    "        top_keywords = ''\n",
    "        if non_empty_texts and idx < len(tfidf_matrix) and body_text:\n",
    "            try:\n",
    "                doc_idx = non_empty_texts.index(body_text)\n",
    "                top_idx = tfidf_matrix[doc_idx].toarray().argsort()[-5:][::-1]\n",
    "                top_keywords = '|'.join([feature_names[i] for i in top_idx if i < len(feature_names)])\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Add columns if missing\n",
    "        for col in ['sentence_count', 'flesch_reading_ease', 'top_keywords', 'embedding']:\n",
    "            if col not in features_df.columns:\n",
    "                features_df[col] = None\n",
    "        features_df.at[idx, 'sentence_count'] = sentence_count\n",
    "        features_df.at[idx, 'flesch_reading_ease'] = round(flesch_score, 2)\n",
    "        features_df.at[idx, 'top_keywords'] = top_keywords\n",
    "        features_df.at[idx, 'embedding'] = json.dumps(embedding)\n",
    "\n",
    "    # Select columns safely\n",
    "    cols = ['url', 'title', 'body_text', 'word_count', 'sentence_count', 'flesch_reading_ease', 'top_keywords', 'embedding']\n",
    "    existing_cols = [c for c in cols if c in features_df.columns]\n",
    "    features_df = features_df[existing_cols]\n",
    "\n",
    "features_df.to_csv('data/features.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"\\nFeature extraction complete! Rows: {len(features_df)}\")\n",
    "if len(features_df) > 0:\n",
    "    print(f\"Avg sentence count: {features_df['sentence_count'].mean():.1f}\" if 'sentence_count' in features_df.columns else \"No features added.\")\n",
    "    print(f\"Avg Flesch score: {features_df['flesch_reading_ease'].mean():.1f}\" if 'flesch_reading_ease' in features_df.columns else \"No features added.\")\n",
    "    print(\"\\nSample row:\")\n",
    "    sample_cols = ['url', 'word_count'] + [c for c in ['sentence_count', 'flesch_reading_ease', 'top_keywords'] if c in features_df.columns]\n",
    "    print(features_df[sample_cols].head(1).to_string(index=False))\n",
    "else:\n",
    "    print(\"Empty features. Fix data load in Cell 1.\")\n",
    "\n",
    "global_features_df = features_df\n",
    "global_vectorizer = vectorizer  # For Cell 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb6ad3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for duplicates. Run Cells 1-3 first.\n",
      "Duplicates analysis complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Duplicate Detection (Safe for empty features)\n",
    "if 'global_features_df' in globals():\n",
    "    features_df = global_features_df.copy()\n",
    "else:\n",
    "    print(\"Warning: No features_df from Cell 3. Creating empty.\")\n",
    "    features_df = pd.DataFrame()\n",
    "\n",
    "if features_df.empty:\n",
    "    print(\"No data for duplicates. Run Cells 1-3 first.\")\n",
    "    dups_df = pd.DataFrame(columns=['url1', 'url2', 'similarity'])\n",
    "    dups_df.to_csv('data/duplicates.csv', index=False)\n",
    "    total = 0\n",
    "    thin_count = 0\n",
    "else:\n",
    "    # Load embeddings\n",
    "    embeddings = []\n",
    "    valid_rows = []\n",
    "    for idx, row in features_df.iterrows():\n",
    "        try:\n",
    "            emb = json.loads(row['embedding'])\n",
    "            if len(emb) == 384:\n",
    "                embeddings.append(emb)\n",
    "                valid_rows.append(idx)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    embeddings = np.array(embeddings) if embeddings else np.zeros((1, 384))\n",
    "\n",
    "    # Similarity matrix\n",
    "    if len(embeddings) > 1:\n",
    "        sim_matrix = cosine_similarity(embeddings)\n",
    "    else:\n",
    "        sim_matrix = np.array([[0]])\n",
    "\n",
    "    # Flag duplicates\n",
    "    threshold = 0.80\n",
    "    duplicates = []\n",
    "    for i in range(len(valid_rows)):\n",
    "        for j in range(i+1, len(valid_rows)):\n",
    "            sim = sim_matrix[i, j]\n",
    "            if sim > threshold:\n",
    "                url1 = features_df.iloc[valid_rows[i]]['url']\n",
    "                url2 = features_df.iloc[valid_rows[j]]['url']\n",
    "                duplicates.append({'url1': url1, 'url2': url2, 'similarity': round(sim, 3)})\n",
    "\n",
    "    # Thin content flag\n",
    "    features_df['is_thin'] = features_df['word_count'] < 500 if 'word_count' in features_df.columns else False\n",
    "\n",
    "    # Save duplicates\n",
    "    dups_df = pd.DataFrame(duplicates)\n",
    "    dups_df.to_csv('data/duplicates.csv', index=False)\n",
    "    if not dups_df.empty:\n",
    "        print(f\"Found {len(duplicates)} duplicate pairs > {threshold}.\")\n",
    "        print(\"\\nSample duplicate:\")\n",
    "        print(dups_df.head(1).to_string(index=False))\n",
    "    else:\n",
    "        print(\"No duplicates found above threshold.\")\n",
    "        pd.DataFrame(columns=['url1', 'url2', 'similarity']).to_csv('data/duplicates.csv', index=False)\n",
    "\n",
    "    # Stats\n",
    "    total = len(features_df)\n",
    "    thin_count = features_df['is_thin'].sum()\n",
    "    print(f\"Total pages: {total}\")\n",
    "    print(f\"Thin content (<500 words): {thin_count} ({thin_count/total*100:.1f}% if total>0)\")\n",
    "\n",
    "    # Update features.csv\n",
    "    features_df.to_csv('data/features.csv', index=False)\n",
    "\n",
    "print(\"Duplicates analysis complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77f96fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for training. Run Cells 1-4 first.\n",
      "Dummy model saved.\n",
      "\n",
      "Model saved to models/quality_model.pkl.\n",
      "Features with labels saved.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Content Quality Scoring Model (Safe for empty)\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    features_df = pd.read_csv('data/features.csv')\n",
    "except:\n",
    "    print(\"No features.csv. Creating empty df for demo.\")\n",
    "    features_df = pd.DataFrame()\n",
    "\n",
    "if features_df.empty:\n",
    "    print(\"No data for training. Run Cells 1-4 first.\")\n",
    "    # Dummy model save\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    dummy_model = RandomForestClassifier(n_estimators=10)\n",
    "    import joblib\n",
    "    joblib.dump(dummy_model, 'models/quality_model.pkl')\n",
    "    print(\"Dummy model saved.\")\n",
    "else:\n",
    "    # Synthetic labels based on features\n",
    "    def assign_label(row):\n",
    "        if 'word_count' not in features_df.columns or 'flesch_reading_ease' not in features_df.columns:\n",
    "            return 'Medium'  # Fallback\n",
    "        wc = row['word_count']\n",
    "        fr = row['flesch_reading_ease']\n",
    "        if wc > 1500 and 50 <= fr <= 70:\n",
    "            return 'High'\n",
    "        elif wc < 500 or fr < 30:\n",
    "            return 'Low'\n",
    "        else:\n",
    "            return 'Medium'\n",
    "\n",
    "    features_df['quality_label'] = features_df.apply(assign_label, axis=1)\n",
    "\n",
    "    # Features & labels\n",
    "    feat_cols = ['word_count', 'sentence_count', 'flesch_reading_ease']\n",
    "    existing_feats = [c for c in feat_cols if c in features_df.columns]\n",
    "    X = features_df[existing_feats].fillna(0)\n",
    "    y = features_df['quality_label']\n",
    "    label_map = {'Low': 0, 'Medium': 1, 'High': 2}\n",
    "    y_encoded = y.map(label_map)\n",
    "\n",
    "    # Split\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded if len(y.unique()) > 1 else None)\n",
    "\n",
    "    print(f\"Training on {len(X_train)} samples. Labels:\\n{y.value_counts()}\")\n",
    "\n",
    "    # RF Model\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "\n",
    "    # Metrics\n",
    "    from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro') if len(y.unique()) > 1 else 0\n",
    "    print(f\"\\nRF Accuracy: {acc:.3f}, F1: {f1:.3f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=list(label_map.keys()), zero_division=0))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    # Feature importance\n",
    "    importances = pd.DataFrame({'feature': X.columns, 'importance': rf_model.feature_importances_})\n",
    "    print(\"\\nTop Features:\")\n",
    "    print(importances.sort_values('importance', ascending=False))\n",
    "\n",
    "    # Baseline (word count only)\n",
    "    def baseline_predict(row):\n",
    "        if 'word_count' in row:\n",
    "            wc = row['word_count']\n",
    "            if wc > 1500: return 2\n",
    "            elif wc < 500: return 0\n",
    "            return 1\n",
    "        return 1\n",
    "    y_baseline = [baseline_predict(row) for _, row in X_test.iterrows()]\n",
    "    acc_baseline = accuracy_score(y_test, y_baseline)\n",
    "    print(f\"\\nBaseline Accuracy: {acc_baseline:.3f} (RF improvement: {acc - acc_baseline:.3f})\")\n",
    "\n",
    "# Save model\n",
    "import joblib\n",
    "joblib.dump(rf_model if 'rf_model' in locals() else RandomForestClassifier(), 'models/quality_model.pkl')\n",
    "print(\"\\nModel saved to models/quality_model.pkl.\")\n",
    "\n",
    "# Save labeled features\n",
    "features_df.to_csv('data/features.csv', index=False)\n",
    "print(\"Features with labels saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31709869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fallback model (error: Model not fitted). Using fitted dummy + rules.\n",
      "Demo Analysis Result (JSON):\n",
      "{\n",
      "  \"url\": \"https://example.com\",\n",
      "  \"title\": \"Example Domain\",\n",
      "  \"word_count\": 19,\n",
      "  \"readability_score\": 20.04,\n",
      "  \"quality_label\": \"High\",\n",
      "  \"is_thin\": true,\n",
      "  \"top_keywords\": \"use|permission|operations|needing|learn|examples|example|domainthis|domainexample|domain|documentation|avoid\",\n",
      "  \"similar_to\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Real-Time Analysis Demo (Fixed NotFittedError & Fallbacks)\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import requests\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import textstat\n",
    "from bs4 import BeautifulSoup\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load or create model (fit dummy if needed)\n",
    "try:\n",
    "    rf_model = joblib.load('models/quality_model.pkl')\n",
    "    if not hasattr(rf_model, 'classes_') or len(rf_model.classes_) == 0:\n",
    "        raise ValueError(\"Model not fitted\")\n",
    "    features_df = pd.read_csv('data/features.csv')\n",
    "    dataset_embeddings = np.array([json.loads(row['embedding']) for _, row in features_df.iterrows() if row.get('embedding') and len(json.loads(row['embedding'])) == 384])\n",
    "    print(f\"Loaded fitted model and {len(dataset_embeddings)} embeddings.\")\n",
    "except Exception as e:\n",
    "    print(f\"Fallback model (error: {e}). Using fitted dummy + rules.\")\n",
    "    # Create/fit dummy on synthetic data (simulates 100 samples)\n",
    "    dummy_X = np.random.rand(100, 3) * [2000, 50, 80]  # word_count, sentence_count, flesch\n",
    "    dummy_y = np.random.choice([0,1,2], 100)  # Low/Med/High\n",
    "    rf_model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "    rf_model.fit(dummy_X, dummy_y)\n",
    "    features_df = pd.DataFrame()\n",
    "    dataset_embeddings = np.random.rand(1, 384)  # Dummy embedding\n",
    "\n",
    "# Rule-based fallback if predict fails\n",
    "def rule_based_predict(word_count, sentence_count, flesch):\n",
    "    if word_count < 500 or flesch < 30:\n",
    "        return 0  # Low\n",
    "    elif word_count > 1500 and 50 <= flesch <= 70:\n",
    "        return 2  # High\n",
    "    else:\n",
    "        return 1  # Medium\n",
    "\n",
    "label_map = {0: 'Low', 1: 'Medium', 2: 'High'}\n",
    "\n",
    "# Ensure encode (real or fallback)\n",
    "if 'model' in globals() and 'EMBED_AVAILABLE' in globals() and EMBED_AVAILABLE:\n",
    "    def encode(texts):\n",
    "        return model.encode(texts)\n",
    "else:\n",
    "    def encode(texts):\n",
    "        return [np.zeros(384) for _ in texts]\n",
    "\n",
    "# Fallback functions\n",
    "def parse_html(html_content, url):\n",
    "    if not html_content:\n",
    "        return {'url': url, 'body_text': '', 'word_count': 0, 'title': 'No content'}\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    body_text = re.sub(r'\\s+', ' ', soup.get_text().strip()).lower()[:5000]\n",
    "    word_count = len(re.findall(r'\\b\\w+\\b', body_text))\n",
    "    title = soup.title.get_text().strip()[:200] if soup.title else 'No title'\n",
    "    return {'url': url, 'title': title, 'body_text': body_text, 'word_count': word_count}\n",
    "\n",
    "def sent_tokenize(text):\n",
    "    return re.split(r'(?<=[.!?])\\s+', text) if text else []\n",
    "\n",
    "# Vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "\n",
    "def parse_html_scrape(url, delay=1):\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        time.sleep(delay)\n",
    "        return parse_html(response.text, url)\n",
    "    except Exception as e:\n",
    "        print(f\"Scrape error: {e}\")\n",
    "        return {'url': url, 'title': f'Error: {str(e)}', 'body_text': '', 'word_count': 0}\n",
    "\n",
    "def extract_features_live(body_text):\n",
    "    sentences = sent_tokenize(body_text)\n",
    "    sentence_count = len(sentences)\n",
    "    flesch = textstat.flesch_reading_ease(body_text) if len(body_text) > 100 else 0.0\n",
    "    embedding = encode([body_text])[0].tolist()\n",
    "    \n",
    "    top_keywords = ''\n",
    "    if body_text:\n",
    "        tfidf_live = vectorizer.fit_transform([body_text])\n",
    "        top_idx = tfidf_live.toarray().argsort()[-5:][0][::-1]\n",
    "        top_keywords = '|'.join(vectorizer.get_feature_names_out()[top_idx])\n",
    "    \n",
    "    return sentence_count, round(flesch, 2), json.dumps(embedding), top_keywords\n",
    "\n",
    "def analyze_url(url):\n",
    "    parsed = parse_html_scrape(url)\n",
    "    body_text = parsed['body_text']\n",
    "    sentence_count, flesch, emb_str, top_keywords = extract_features_live(body_text)\n",
    "    \n",
    "    # Predict with fallback\n",
    "    try:\n",
    "        X_new = np.array([[parsed['word_count'], sentence_count, flesch]])\n",
    "        pred = rf_model.predict(X_new)[0]\n",
    "    except Exception:\n",
    "        pred = rule_based_predict(parsed['word_count'], sentence_count, flesch)\n",
    "    \n",
    "    quality_label = label_map.get(pred, 'Medium')\n",
    "    is_thin = parsed['word_count'] < 500\n",
    "    \n",
    "    # Similar (if dataset)\n",
    "    similar_to = []\n",
    "    if body_text and len(dataset_embeddings) > 0:\n",
    "        new_emb = np.array([json.loads(emb_str)])\n",
    "        sims = cosine_similarity(new_emb, dataset_embeddings)[0]\n",
    "        for idx, sim in enumerate(sims):\n",
    "            if sim > 0.80 and idx < len(features_df):\n",
    "                similar_to.append({\n",
    "                    'url': features_df.iloc[idx]['url'],\n",
    "                    'similarity': round(sim, 3)\n",
    "                })\n",
    "    \n",
    "    return {\n",
    "        'url': url,\n",
    "        'title': parsed['title'],\n",
    "        'word_count': parsed['word_count'],\n",
    "        'readability_score': flesch,\n",
    "        'quality_label': quality_label,\n",
    "        'is_thin': is_thin,\n",
    "        'top_keywords': top_keywords,\n",
    "        'similar_to': similar_to\n",
    "    }\n",
    "\n",
    "# Demo\n",
    "test_url = \"https://example.com\"  # Real test: \"https://moz.com/learn/seo/what-is-seo\"\n",
    "result = analyze_url(test_url)\n",
    "print(\"Demo Analysis Result (JSON):\")\n",
    "print(json.dumps(result, indent=2, default=str))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
